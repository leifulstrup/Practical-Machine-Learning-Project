---
title: "Practical-Machine-Learning-July-2014-LeifUlstrup.Rmd"
author: "Leif Ulstrup"
date: "July 24, 2014"
output: html_document
---
##Executive Summary

##Context

Source of Experimental Data: http://groupware.les.inf.puc-rio.br/har (see section "Weight Lifting Exercises Dataset"").  See this paper: http://groupware.les.inf.puc-rio.br/work.jsf?p1=11201.  From the paper:
"...Participants were asked to perform one set of 10 repetitions
of the Unilateral Dumbbell Biceps Curl in Five different fashions:
exactly according to the specification (Class A), throwing
the elbows to the front (Class B), lifting the dumbbell
only halfway (Class C), lowering the dumbbell only halfway
(Class D) and throwing the hips to the front (Class E). Class
A corresponds to the specified execution of the exercise,
while the other 4 classes correspond to common mistakes..."

##Model Development

###How the Model was Built

###How Cross-validation was Used

###Expected Sample Error Calculations

Confusion Matrix and Statistics

          Reference
Prediction    A    B    C    D    E
         A 1667    2    3    1    1
         B   27 1088   23    0    1
         C    0   14 1003    9    0
         D    3    1   37  921    2
         E    0    2   10   10 1060

Overall Statistics
                                         
               Accuracy : 0.9752         
                 95% CI : (0.9709, 0.979)
    No Information Rate : 0.2884         
    P-Value [Acc > NIR] : < 2.2e-16      
                                         
                  Kappa : 0.9686         
 Mcnemar's Test P-Value : 1.242e-09      

Statistics by Class:

                     Class: A Class: B Class: C Class: D Class: E
Sensitivity            0.9823   0.9828   0.9322   0.9787   0.9962
Specificity            0.9983   0.9893   0.9952   0.9913   0.9954
Pos Pred Value         0.9958   0.9552   0.9776   0.9554   0.9797
Neg Pred Value         0.9929   0.9960   0.9850   0.9959   0.9992
Prevalence             0.2884   0.1881   0.1828   0.1599   0.1808
Detection Rate         0.2833   0.1849   0.1704   0.1565   0.1801
Detection Prevalence   0.2845   0.1935   0.1743   0.1638   0.1839
Balanced Accuracy      0.9903   0.9861   0.9637   0.9850   0.9958

###Reasoning Behind Choices Made

###Link to Predictions

###R Code

```{r loadLibraries, echo=FALSE}
require(caret)

install.packages("doMC") # install parallel processing library
require(doMC)
registerDoMC(cores = 4)

require("randomForest") # for model training builds

```

```{r ingestData, echo=FALSE}

#assume that the working directory has been set correctly
#download source files to current working directory
# commented out for this effort since they are already downloaded (remove otherwise to test)
#download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", "pml-training.csv", method="curl")
#download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", "pml-testing.csv", method="curl")
if(!("pml-testing.csv" %in% list.files())) stop("Code Stopped: Missing testing.csv file")
if(!("pml-training.csv" %in% list.files())) stop("Code Stopped: Missing testing.csv file")

#test files are in the directory

trainingFile <- read.csv("pml-training.csv") # source file for training and testing fit
testFile <- read.csv("pml-testing.csv") # ultimate test file for project submission

```

```{r dataCleanup, echo=FALSE}
# remove nearZero Values and NAs

# match the columns in the training source to those used in the test set
keepcols <- c()
for (i in names(testFile)) {
  if(!is.na(testFile[[i]][1])){
    #print(paste(i, "is NOT NA"))
    keepcols <- c(keepcols, i)
    }
  }

trainingClean <- data.frame()
trainingClean <- trainingFile[, (names(trainingFile) %in% keepcols)]
trainingClean$classe <- trainingFile$classe # indicator of how well the bar bell movement is performed on a scale of A to E

testClean <- data.frame()
testClean <- testFile[, (names(trainingFile) %in% keepcols)]
testClean$problem_id <- testFile$problem_id # note that the last column is "problem_id" instead of "classe"

trainingClean <- trainingClean[-c(1:7)] #remove the names, timestamp etc columns
testClean <- testClean[-c(1:7)] #remove the names, timestamp etc columns

#convert to "numeric" class except for the classe factor
for (i in 1:(length(trainingClean)-1)) {trainingClean[[i]] <- as.numeric(trainingClean[[i]])}
for (i in 1:(length(testClean)-1)) {testClean[[i]] <- as.numeric(testClean[[i]])}

```

```{r createTrainingandTest, echo=FALSE}

inTrain <- createDataPartition(y=trainingClean$classe, p=0.7, list=FALSE) # Note use of "clean" version of trainingFile
training <- trainingClean[inTrain,] # Note use of "clean" version of trainingFile
testing <- trainingClean[-inTrain,] # Note use of "clean" version of trainingFile

# !!! huge roadblock solved - another clean up item (per Class Forum post)
#  see this stackoverflow discussion:
# http://stackoverflow.com/questions/13495041/random-forests-in-r-empty-classes-in-y-and-argument-legth-0
training$classe <- factor(training$classe) #key cleanup actvity
testing$classe <- factor(testing$classe) #key cleanup actvity

# we are now set up with a set of data we can train and test our models on...let's try some models

```

```{r analyzeCorrelations, echo=FALSE}

trainingCor <- cor(training[, -length(training)]) # account for the classe factor on last column
highCor <- sum(abs(training[upper.tri(trainingCor)]) > 0.999)

highlyCor <- findCorrelation(training[, -length(training)], cutoff = 0.75)

preProc <- preProcess(training[, -length(training)], method= c("center", "scale"))

trainPredict <- predict(preProc, newdata = training[, -length(training)])

```

```{r modeling, echo=FALSE}
set.seed(12345)

# let's try a model using 

preProc <- preProcess(training[,-53], method="pca") # "-53" removes the $classe factor element
trainPC <- predict(preProc, training[,-53]) # note again extraction of $classe
modelFit <- train(training$classe ~ ., method="rf", data=trainPC) # strange construct, rf for RandomForest
print(modelFit)
testPC <- predict(preProc, testing[,-53])
confusionMatrix(testing$classe, predict(modelFit, testPC))

# Now, let's test this on the testClean data that came from the testing file
# that file as 53 cols and 20 rows and the results of that feed into the HW assignment
classTestPC <- predict(preProc, testClean[,-53]) # remove $problem_id = $classe
classTestPredictedValues <- predict(modelFit, classTestPC) # vector of predicted factors A-E
classTestPredictedValues <- as.character(classTestPredictedValues)

# code from Prediction Assignment Submission: Instructions
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
    }
  }
pml_write_files(classTestPredictedValues) # this wites out the 20 answers to 20 files for individual upload
  
```
